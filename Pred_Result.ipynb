{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiYe-bB2TyJh"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT4t7cMYTUn3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from scipy import stats\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cltWYpsUf_xF"
   },
   "source": [
    "## Fungsi untuk menampilkan gambar dari folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm8nmzr0cNyZ"
   },
   "outputs": [],
   "source": [
    "def show_images_from_folder(folder_path, title=\"Sample Images\"):\n",
    "    images = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "    if len(images) == 0:\n",
    "        print(f\"Tidak ada gambar di folder {folder_path}\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(3):\n",
    "        img = cv2.imread(images[i], cv2.IMREAD_GRAYSCALE)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(os.path.basename(images[i]))\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmcRkqldgbDZ"
   },
   "source": [
    "## Membuat folder untuk menyimpan gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5EMgLy7gDog"
   },
   "outputs": [],
   "source": [
    "filter_folder = \"filtered_images\"\n",
    "clahe_folder = \"filtered_clahe\"\n",
    "output_folder = \"output_images\"\n",
    "\n",
    "# Cek folder gambar yang sudah ada\n",
    "for folder in [filter_folder, clahe_folder, output_folder]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgW4rsMgymS"
   },
   "source": [
    "## Preprocessing: Gaussian filter dan CLACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03zTY-d9nllk"
   },
   "outputs": [],
   "source": [
    "def preprocessing(img, kernel_size=(5,5)):\n",
    "    if img is None:\n",
    "        return None\n",
    "    # Mengaplikasikan Gaussian\n",
    "    filtered_img = cv2.GaussianBlur(img, kernel_size, 0.5)\n",
    "\n",
    "    # Mengaplikasikan CLAHE untuk contrast enhancement\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced_img = clahe.apply(filtered_img)\n",
    "\n",
    "    return enhanced_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax5gqwB7k8IL"
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr7W00zMlTXo"
   },
   "source": [
    "### Otsu thresholding (sensitive segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18vJtTVahiFR"
   },
   "outputs": [],
   "source": [
    "def otsu_threshold(image):\n",
    "  if image is None:\n",
    "    return None\n",
    "  _, otsu_thresh_img = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "  return otsu_thresh_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq0D716cm9Fy"
   },
   "source": [
    "### Global thresholding (insensitive segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5j3TRlFmKrU"
   },
   "outputs": [],
   "source": [
    "def global_threshold(image, threshold_value=127):\n",
    "  if image is None:\n",
    "    return None\n",
    "  _, global_thresh_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "  return global_thresh_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEnad8eHnFd1"
   },
   "source": [
    "### Morphological processing (closing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx8P6weonH2i"
   },
   "outputs": [],
   "source": [
    "def closing(binary_mask, kernel_size=(5, 5)):\n",
    "  if binary_mask is None:\n",
    "    return None\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "  closed = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
    "  return closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FTuA5fx_GbY"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT5VYeKs_ZBw"
   },
   "source": [
    "### Fitur bentuk (shape fearures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSJK9P_o_UMq"
   },
   "outputs": [],
   "source": [
    "def extract_shape_features(mask):\n",
    "  if mask is None:\n",
    "    return {f'shape_{i}': 0 for i in range(17)} # Return default features if mask is None\n",
    "\n",
    "  # Memberikan label ke komponen yang terhubung\n",
    "  labelled_mask = measure.label(mask > 0)\n",
    "  regions = measure.regionprops(labelled_mask)\n",
    "\n",
    "  if len(regions) == 0:\n",
    "    return {f'shape_{i}': 0 for i in range(17)}\n",
    "\n",
    "  # Mengurutkan region berdasarkan area (asumsikan 2 region dengan area terbesar adalah paru-paru kanan dan kiri)\n",
    "  regions = sorted(regions, key=lambda x: x.area, reverse=True)[:2]\n",
    "\n",
    "  features = {}\n",
    "\n",
    "  if len(regions) >= 2:\n",
    "    r_left, r_right = regions[0], regions[1]\n",
    "\n",
    "    # Fitur 1: tinggi\n",
    "    h_l = r_left.bbox[2] - r_left.bbox[0]\n",
    "    h_r = r_right.bbox[2] - r_right.bbox[0]\n",
    "    features['height_left'] = h_l\n",
    "    features['height_right'] = h_r\n",
    "    features['height_diff'] = abs(h_l - h_r)\n",
    "\n",
    "    # Fitur 2: rasio lebar/tinggi (simplified)\n",
    "    # Ensure h_l and h_r are not zero to prevent division by zero\n",
    "    features['ratio_left_0'] = r_left.major_axis_length / (h_l + 1e-5) if h_l > 0 else 0\n",
    "    features['ratio_right_0'] = r_right.major_axis_length / (h_r + 1e-5) if h_r > 0 else 0\n",
    "    # Only need one ratio for now, or loop it properly. Simplified for existing code structure.\n",
    "    # The original loop for i in range(5) with same values was likely a placeholder or error.\n",
    "    for i in range(1, 5):\n",
    "        features[f'ratio_left_{i}'] = features['ratio_left_0']\n",
    "        features[f'ratio_right_{i}'] = features['ratio_right_0']\n",
    "\n",
    "    # Fitur 3: perimeter (keliling)\n",
    "    features['perimeter_left'] = r_left.perimeter\n",
    "    features['perimeter_right'] = r_right.perimeter\n",
    "\n",
    "    # Fitur 4: eccentricity (\"keanehan\")\n",
    "    features['eccentricity_left'] = r_left.eccentricity\n",
    "    features['eccentricity_right'] = r_right.eccentricity\n",
    "    return features\n",
    "\n",
    "  else: # jika hanya ada 1 region yang terdeteksi\n",
    "    r = regions[0]\n",
    "    h = r.bbox[2] - r.bbox[0]\n",
    "    features['height_left'] = h\n",
    "    features['height_right'] = 0\n",
    "    features['height_diff'] = h\n",
    "\n",
    "    features['ratio_left_0'] = r.major_axis_length / (h + 1e-5) if h > 0 else 0\n",
    "    for i in range(1, 5):\n",
    "        features[f'ratio_left_{i}'] = features['ratio_left_0']\n",
    "        features[f'ratio_right_{i}'] = 0\n",
    "\n",
    "    features['perimeter_left'] = r.perimeter\n",
    "    features['perimeter_right'] = 0\n",
    "\n",
    "    features['eccentricity_left'] = r.eccentricity\n",
    "    features['eccentricity_right'] = 0\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fepE27mccoE6"
   },
   "source": [
    "### First-order Statistical Features (FOFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOFtjODhnfDp"
   },
   "outputs": [],
   "source": [
    "def fofs(image, mask):\n",
    "  # Add initial None checks for image and mask\n",
    "  if image is None or mask is None:\n",
    "    return {k: 0 for k in ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'entropy', 'smoothness', 'uniformity']}\n",
    "\n",
    "  # Apply mask ke gambar\n",
    "  masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "  # Check if masked_image has valid pixels or if the mask is completely empty\n",
    "  if np.all(masked_image == 0) or not np.any(mask > 0):\n",
    "    return {k: 0 for k in ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'entropy', 'smoothness', 'uniformity']}\n",
    "\n",
    "  pixels = masked_image[mask > 0]\n",
    "\n",
    "  if len(pixels) == 0:\n",
    "    return {k: 0 for k in ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'entropy', 'smoothness', 'uniformity']}\n",
    "\n",
    "  # Menghitung fitur statistik\n",
    "  mean = np.mean(pixels)\n",
    "  variance = np.var(pixels)\n",
    "  std_dev = np.std(pixels)\n",
    "  skewness = stats.skew(pixels)\n",
    "  kurtosis = stats.kurtosis(pixels)\n",
    "\n",
    "  # Fitur histogram\n",
    "  hist, _ = np.histogram(pixels, bins=255, range=(0, 256), density=True)\n",
    "  hist = hist[hist > 0]\n",
    "  entropy = -np.sum(hist * np.log2(hist + 1e-10)) # Corrected line\n",
    "  smoothness = 1 - (1 / (1 + variance))\n",
    "  uniformity = np.sum(hist ** 2)\n",
    "\n",
    "  return {\n",
    "      'mean' : mean,\n",
    "      'variance' : variance,\n",
    "      'std_dev' : std_dev,\n",
    "      'skewness' : skewness,\n",
    "      'kurtosis': kurtosis,\n",
    "      'entropy' : entropy,\n",
    "      'smoothness' : smoothness,\n",
    "      'uniformity' : uniformity\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25TfDXesh6Gd"
   },
   "source": [
    "### Gray-Level Co-occurence Matrix (GLCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwUMut1igMEv"
   },
   "outputs": [],
   "source": [
    "def glcm(image, mask):\n",
    "  # Add initial None checks for image and mask\n",
    "  if image is None or mask is None:\n",
    "    return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  # Apply mask to the image\n",
    "  masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "  # Check if masked_image is effectively empty (all zeros) after masking\n",
    "  if np.all(masked_image == 0):\n",
    "      return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  # Normalize the image to 8-bit. Handle cases where normalization might be problematic.\n",
    "  # Ensure there are non-zero pixels before attempting normalization based on min/max.\n",
    "  min_val = np.min(masked_image[masked_image > 0]) if np.any(masked_image > 0) else 0\n",
    "  max_val = np.max(masked_image) if np.any(masked_image > 0) else 0\n",
    "\n",
    "  # If no meaningful pixel data (e.g., all pixels are the same or zero after mask),\n",
    "  # return default features.\n",
    "  if max_val == 0 or min_val == max_val:\n",
    "      return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  # Perform normalization safely\n",
    "  img_8bit = cv2.normalize(masked_image, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "\n",
    "  # GLCM in 4 directions (0, 45, 90, 135)\n",
    "  distances = [1]\n",
    "  angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "  # Check if img_8bit is suitable for graycomatrix (needs at least 2 unique gray levels for meaningful GLCM)\n",
    "  if img_8bit.size == 0 or len(np.unique(img_8bit)) < 2:\n",
    "      return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  glcm = graycomatrix(img_8bit, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "  # Extract properties\n",
    "  features = {}\n",
    "  properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "\n",
    "  for prop in properties:\n",
    "    values = graycoprops(glcm, prop)\n",
    "    for i, angle in enumerate(['0', '45', '90', '135']):\n",
    "      features[f'glcm_{prop}_{angle}'] = values[0, i]\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPqth8HFpVhS"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So8veWL6mGgA"
   },
   "outputs": [],
   "source": [
    "def process_single_image(image_path):\n",
    "  if not isinstance(image_path, str):\n",
    "    print(\"Invalid path:\", image_path)\n",
    "    return None\n",
    "\n",
    "  if not os.path.exists(image_path):\n",
    "    print(\"Path not found:\", image_path)\n",
    "    return None\n",
    "\n",
    "  # load\n",
    "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "  if img is None:\n",
    "    print(\"OpenCV cannot read:\", image_path)\n",
    "    return None\n",
    "\n",
    "  # 1. Preprocessing\n",
    "  # Pass the loaded image 'img' to preprocessing to avoid redundant loading\n",
    "  enhanced = preprocessing(img)\n",
    "  if enhanced is None:\n",
    "    print(f\"Preprocessing failed for {image_path}\")\n",
    "    return None\n",
    "\n",
    "  # 2. Segmentation (Sensitive)\n",
    "  otsu_mask = otsu_threshold(enhanced)\n",
    "  if otsu_mask is None:\n",
    "    print(f\"Otsu thresholding failed for {image_path}\")\n",
    "    return None\n",
    "  closed_otsu = closing(otsu_mask)\n",
    "  if closed_otsu is None:\n",
    "    print(f\"Closing (Otsu) failed for {image_path}\")\n",
    "    return None\n",
    "\n",
    "  # 3. Segmentation (Insensitive)\n",
    "  global_mask = global_threshold(enhanced)\n",
    "  if global_mask is None:\n",
    "    print(f\"Global thresholding failed for {image_path}\")\n",
    "    return None\n",
    "  closed_global = closing(global_mask)\n",
    "  if closed_global is None:\n",
    "    print(f\"Closing (Global) failed for {image_path}\")\n",
    "    return None\n",
    "\n",
    "  # 4. Feature extraction\n",
    "  # These functions are now expected to return dictionaries (possibly with default/zero values) or handle their own None inputs\n",
    "  shape_featues = extract_shape_features(closed_otsu)\n",
    "  fofs_features = fofs(enhanced, closed_global)\n",
    "  glcm_features = glcm(enhanced, closed_global)\n",
    "\n",
    "  # Gabungkan semua fitur - now safer as all feature functions should return dicts\n",
    "  all_features = {**shape_featues, **fofs_features, **glcm_features}\n",
    "\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_n9nzsxqpCo"
   },
   "outputs": [],
   "source": [
    "def extract_dataset(df, limit=None):\n",
    "  features_list = []\n",
    "  labels = []\n",
    "\n",
    "  total = len(df) if limit is None else min(limit, len(df))\n",
    "\n",
    "  for idx, row in df.iterrows():\n",
    "    if limit and idx >= limit:\n",
    "      break\n",
    "\n",
    "    if idx % 50 == 0:\n",
    "      print(f\"Processing image {idx+1}/{total}...\")\n",
    "\n",
    "    features = process_single_image(row['path'])\n",
    "    if features is not None:\n",
    "      features_list.append(features)\n",
    "      labels.append(row['label'])\n",
    "\n",
    "  # Konversi ke dataframe\n",
    "  features_df = pd.DataFrame(features_list)\n",
    "  features_df['label'] = labels\n",
    "\n",
    "  print(f\"\\nFeature extraction selesai!\")\n",
    "  print(f\"Total fitur yang diekstrak: {len(features_df.columns)-1}\")\n",
    "\n",
    "  return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_7xPR7EL6Wp"
   },
   "source": [
    "### Klasifikasi hierarkis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtGdRPdkL5YO"
   },
   "outputs": [],
   "source": [
    "class klasifikasiHierarkis:\n",
    "  \"\"\"\n",
    "  Implementasi:\n",
    "  Stage 1: Klasifikasi berdasarkan fitur-fitur bentuk (shape features)\n",
    "  Stage 2: Jika stage 1 = normal, cek dengan semua fitur\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model_type='svm'):\n",
    "    self.model_type = model_type\n",
    "    self.stage1_model = None # Shape feature\n",
    "    self.stage2_model = None # Semua fitur\n",
    "    self.scaler1 = StandardScaler()\n",
    "    self.scaler2 = StandardScaler()\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"\n",
    "    Training model secara dierarkis\n",
    "    X: Dataframe dengan semua fitur\n",
    "    y: label\n",
    "    \"\"\"\n",
    "    # Identifikasi kolom shape features\n",
    "    shape_cols = [col for col in X.columns if 'height' in col or\n",
    "                  'ratio' in col or 'perimeter' in col or 'eccentricity' in col]\n",
    "\n",
    "    X_shape = X[shape_cols]\n",
    "    X_all = X\n",
    "\n",
    "    # Fitur skalar\n",
    "    X_shape_scaled = self.scaler1.fit_transform(X_shape)\n",
    "    X_all_scaled = self.scaler2.fit_transform(X_all)\n",
    "\n",
    "    # Training model Stage 1\n",
    "    if self.model_type == 'svm':\n",
    "      self.stage1_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "      self.stage2_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    else:\n",
    "      self.stage1_model = GaussianNB()\n",
    "      self.stage2_model = GaussianNB()\n",
    "\n",
    "    self.stage1_model.fit(X_shape_scaled, y)\n",
    "    self.stage2_model.fit(X_all_scaled, y)\n",
    "\n",
    "    print(f\"Stage 1 ({self.model_type}) trained on {len(shape_cols)} fitur-fitur bentuk (shape features)\")\n",
    "    print(f\"Stage 2 ({self.model_type}) trained on {X_all.shape[1]} semua fitur\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Prediksi Hierarkis\n",
    "    \"\"\"\n",
    "    shape_cols = [col for col in X.columns if 'height' in col or\n",
    "                   'ratio' in col or 'perimeter' in col or 'eccentricity' in col]\n",
    "\n",
    "    X_shape = X[shape_cols]\n",
    "    X_all = X\n",
    "\n",
    "    X_shape_scaled = self.scaler1.transform(X_shape)\n",
    "    X_all_scaled = self.scaler2.transform(X_all)\n",
    "\n",
    "    # Stage 1: Prediksi dengan fitur-fitur bentuk (shape features)\n",
    "    stage1_pred = self.stage1_model.predict(X_shape_scaled)\n",
    "    stage1_proba = self.stage1_model.predict_proba(X_shape_scaled)[:, 1]\n",
    "\n",
    "    # Stage 2: Untuk case yang diprediksi normal, cek lagi dengan semua fitur\n",
    "    final_pred = stage1_pred.copy()\n",
    "    for i in range(len(stage1_pred)):\n",
    "      if stage1_pred[i] == 0:  # Prediksi = normal\n",
    "        # Gunakan hasil stage 2 untuk prediksi final\n",
    "        stage2_pred = self.stage2_model.predict(X_all_scaled[i:i+1])\n",
    "        final_pred[i] = stage2_pred[0]\n",
    "\n",
    "    return final_pred\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    \"\"\"Get probabilitas prediksi\"\"\"\n",
    "    predictions = self.predict(X)\n",
    "    proba = np.zeros((len(predictions), 2))\n",
    "    proba[predictions == 0, 0] = 1\n",
    "    proba[predictions == 1, 1] = 1\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_models(svm_path=\"svm_model.pkl\", nb_path=\"nb_model.pkl\"):\n",
    "    \"\"\"Load pre-trained hierarchical classifiers for inference.\"\"\"\n",
    "    with open(svm_path, \"rb\") as f:\n",
    "        svm_model = pickle.load(f)\n",
    "    with open(nb_path, \"rb\") as f:\n",
    "        nb_model = pickle.load(f)\n",
    "    print(f\"[INFO] Models loaded ← {svm_path}, {nb_path}\")\n",
    "    return svm_model, nb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_feature_alignment(features_df, required_cols):\n",
    "    \"\"\"Ensure all required columns exist and follow the trained order.\"\"\"\n",
    "    missing_cols = [col for col in required_cols if col not in features_df.columns]\n",
    "    if missing_cols:\n",
    "        for col in missing_cols:\n",
    "            features_df[col] = 0\n",
    "    return features_df[required_cols]\n",
    "\n",
    "\n",
    "def calculate_prediction_results(\n",
    "    image_path,\n",
    "    svm_model_path=\"svm_model.pkl\",\n",
    "    nb_model_path=\"nb_model.pkl\",\n",
    "    preloaded_models=None,\n",
    "):\n",
    "    \"\"\"Hitung hasil prediksi TB untuk satu gambar dan tampilkan metrik penting.\"\"\"\n",
    "    if not image_path or not os.path.exists(image_path):\n",
    "        print(f\"⚠️ Path gambar tidak valid: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    features = process_single_image(image_path)\n",
    "    if features is None:\n",
    "        print(\"⚠️ Gagal mengekstrak fitur dari gambar.\")\n",
    "        return None\n",
    "\n",
    "    features_df = pd.DataFrame([features])\n",
    "\n",
    "    # Load trained models (use cached pair when tersedia)\n",
    "    if preloaded_models is not None:\n",
    "        svm_model, nb_model = preloaded_models\n",
    "    else:\n",
    "        try:\n",
    "            svm_model, nb_model = load_models(svm_model_path, nb_model_path)\n",
    "        except FileNotFoundError as err:\n",
    "            print(err)\n",
    "            return None\n",
    "\n",
    "    # Pastikan fitur align untuk tiap model\n",
    "    svm_all_cols = list(svm_model.scaler2.feature_names_in_)\n",
    "    nb_all_cols = list(nb_model.scaler2.feature_names_in_)\n",
    "    svm_inputs = ensure_feature_alignment(features_df.copy(), svm_all_cols)\n",
    "    nb_inputs = ensure_feature_alignment(features_df.copy(), nb_all_cols)\n",
    "\n",
    "    # SVM predictions\n",
    "    svm_pred = svm_model.predict(svm_inputs)[0]\n",
    "    svm_stage2_scaled = svm_model.scaler2.transform(svm_inputs)\n",
    "    svm_tb_prob = svm_model.stage2_model.predict_proba(svm_stage2_scaled)[0, 1]\n",
    "\n",
    "    # NB predictions\n",
    "    nb_pred = nb_model.predict(nb_inputs)[0]\n",
    "    nb_stage2_scaled = nb_model.scaler2.transform(nb_inputs)\n",
    "    nb_tb_prob = nb_model.stage2_model.predict_proba(nb_stage2_scaled)[0, 1]\n",
    "\n",
    "    avg_tb_percent = (svm_tb_prob + nb_tb_prob) / 2 * 100\n",
    "    final_pred = \"TB\" if svm_pred == 1 or nb_pred == 1 else \"Normal\"\n",
    "    label_map = {0: \"Normal\", 1: \"TB\"}\n",
    "\n",
    "    results = {\n",
    "        \"image\": image_path,\n",
    "        \"final_prediction\": final_pred,\n",
    "        \"svm_prediction\": label_map.get(svm_pred, svm_pred),\n",
    "        \"nb_prediction\": label_map.get(nb_pred, nb_pred),\n",
    "        \"svm_tb_probability\": svm_tb_prob * 100,\n",
    "        \"nb_tb_probability\": nb_tb_prob * 100,\n",
    "        \"average_tb_percentage\": avg_tb_percent,\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== TB Prediction Results ===\")\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_folder_predictions(\n",
    "    folder_path,\n",
    "    svm_model_path=\"svm_model.pkl\",\n",
    "    nb_model_path=\"nb_model.pkl\",\n",
    "):\n",
    "    \"\"\"Jalankan prediksi untuk semua gambar dalam satu folder.\"\"\"\n",
    "    if not folder_path:\n",
    "        print(\"⚠️ Path folder tidak boleh kosong.\")\n",
    "        return None\n",
    "\n",
    "    folder = Path(folder_path).expanduser()\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        print(f\"⚠️ Folder tidak ditemukan: {folder}\")\n",
    "        return None\n",
    "\n",
    "    supported_ext = {\".png\", \".jpg\", \".jpeg\"}\n",
    "    image_files = sorted(\n",
    "        [str(p) for p in folder.iterdir() if p.suffix.lower() in supported_ext]\n",
    "    )\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"⚠️ Tidak ada gambar (.png/.jpg) di folder {folder}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        models = load_models(svm_model_path, nb_model_path)\n",
    "    except FileNotFoundError as err:\n",
    "        print(err)\n",
    "        return None\n",
    "\n",
    "    aggregated_results = []\n",
    "    print(f\"\\nMenjalankan prediksi untuk {len(image_files)} gambar di {folder}...\")\n",
    "\n",
    "    for img_path in image_files:\n",
    "        print(f\"\\nProcessing: {img_path}\")\n",
    "        result = calculate_prediction_results(\n",
    "            img_path,\n",
    "            preloaded_models=models,\n",
    "        )\n",
    "        if result is not None:\n",
    "            aggregated_results.append(result)\n",
    "\n",
    "    if aggregated_results:\n",
    "        results_df = pd.DataFrame(aggregated_results)\n",
    "        summary_cols = [\"image\", \"final_prediction\", \"average_tb_percentage\"]\n",
    "        print(\"\\n=== Ringkasan Prediksi Folder ===\")\n",
    "        print(results_df[summary_cols].to_string(index=False))\n",
    "        return results_df\n",
    "\n",
    "    print(\"⚠️ Tidak ada hasil prediksi yang valid.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== KONFIGURASI INPUT USER ===\")\n",
    "    folder_path_raw = input(\n",
    "        \"Masukkan folder yang berisi gambar untuk batch inference (Enter untuk skip): \"\n",
    "    ).strip()\n",
    "    single_image_path_raw = input(\n",
    "        \"Masukkan path gambar tunggal untuk dianalisis (Enter untuk skip): \"\n",
    "    ).strip()\n",
    "\n",
    "    if folder_path_raw:\n",
    "        calculate_folder_predictions(folder_path_raw)\n",
    "    elif single_image_path_raw:\n",
    "        image_path = Path(single_image_path_raw).expanduser()\n",
    "        calculate_prediction_results(str(image_path))\n",
    "    else:\n",
    "        print(\n",
    "            \"Isi path folder atau gambar tunggal agar prediksi dapat dijalankan.\"\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
