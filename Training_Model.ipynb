{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiYe-bB2TyJh"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT4t7cMYTUn3"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from scipy import stats\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"=== KONFIGURASI INPUT USER ===\")\n",
    "dataset_ref = input(\"Masukkan dataset Kaggle (tekan Enter untuk default raddar/tuberculosis-chest-xrays-shenzhen): \").strip() or \"raddar/tuberculosis-chest-xrays-shenzhen\"\n",
    "limit_extract_raw = input(\"Batasi jumlah gambar saat ekstraksi fitur? (Enter untuk semua): \").strip()\n",
    "limit_extract = int(limit_extract_raw) if limit_extract_raw else None\n",
    "user_image_path_raw = input(\"Masukkan path gambar tunggal untuk dianalisis (Enter untuk skip): \").strip()\n",
    "user_image_path = Path(user_image_path_raw).expanduser() if user_image_path_raw else None\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset_ref\": dataset_ref,\n",
    "    \"limit_extract\": limit_extract,\n",
    "    \"user_image_path\": user_image_path,\n",
    "}\n",
    "\n",
    "print(\"\\nRingkasan konfigurasi:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  - {key}: {value if value else 'default/none'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpme0JuAavKI"
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPZr9o-tUsa3",
    "outputId": "81f42e56-09bc-43ca-8c0e-c720a1601aec"
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub\n",
    "\n",
    "# Download dataset langsung dari kaggle berdasarkan input user\n",
    "dataset_ref = CONFIG.get(\"dataset_ref\", \"raddar/tuberculosis-chest-xrays-shenzhen\")\n",
    "print(f\"Mengunduh dataset: {dataset_ref}\")\n",
    "path = kagglehub.dataset_download(dataset_ref)\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "CSzODcauaw-o",
    "outputId": "1c37591a-035b-4d1e-a3ec-cae91162771c"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing_extensions import dataclass_transform\n",
    "\n",
    "# Pastikan variabel `path` dari proses unduh kagglehub sudah ada\n",
    "if 'path' not in locals():\n",
    "    raise RuntimeError(\"Variabel 'path' belum tersedia. Jalankan sel download kagglehub terlebih dahulu.\")\n",
    "\n",
    "# Gunakan folder hasil unduhan kagglehub secara langsung\n",
    "img_path = Path(path)\n",
    "if not img_path.exists():\n",
    "    # Fallback ke lokasi cache default bila diperlukan\n",
    "    fallback_path = Path.home() / \".cache\" / \"kagglehub\" / \"datasets\" / \"raddar\" / \"tuberculosis-chest-xrays-shenzhen\" / \"versions\" / \"1\"\n",
    "    if fallback_path.exists():\n",
    "        img_path = fallback_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Folder dataset tidak ditemukan di {img_path} maupun {fallback_path}\")\n",
    "\n",
    "print(f\"Menggunakan folder dataset: {img_path}\")\n",
    "\n",
    "data = []\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk(img_path):\n",
    "    for file in files:\n",
    "        if file.endswith(('.png')):\n",
    "            data_files.append(os.path.join(root, file))\n",
    "\n",
    "data_files = sorted(data_files)\n",
    "print (\"Banyak gambar pada dataset : \",len(data_files))\n",
    "\n",
    "print(\"\\nMenganalisis filename untuk menentukan label...\")\n",
    "\n",
    "# Parsing labels dari filename\n",
    "normal_count = 0\n",
    "tb_count = 0\n",
    "\n",
    "for filepath in data_files:\n",
    "  basename = os.path.basename(filepath)\n",
    "  # Sistem penamaan dataset Shenzhen: CHNCXR_0xxx_0.png atau CHNCXR_0xxx_1.png\n",
    "  # _0 = Normal, _1 = TB\n",
    "  if '_0.png' in basename:\n",
    "    label = 0\n",
    "    normal_count += 1\n",
    "  elif '_1.png' in basename:\n",
    "    label = 1\n",
    "    tb_count += 1\n",
    "  else:\n",
    "    # Fallback: check kata kunci\n",
    "    basename_lower = basename.lower()\n",
    "    if 'normal' in basename_lower:\n",
    "      label = 0\n",
    "      normal_count += 1\n",
    "    elif 'tb' in basename_lower or 'tuberculosis' in basename_lower:\n",
    "      label = 1\n",
    "      tb_count += 1\n",
    "    else:\n",
    "      # Jika tidak dapat ditentukan, skip gambar\n",
    "      print(f\"  Warning: Tidak dapat menentukan label untuk {basename}, skipping...\")\n",
    "      continue\n",
    "\n",
    "  data.append({'path': filepath, 'label': label})\n",
    "\n",
    "print(f\"  Ditemukan {normal_count} gambar normal\")\n",
    "print(f\"  Ditemukan {tb_count} gambar TB\")\n",
    "\n",
    "if len(data) == 0:\n",
    "  print(\"\\n⚠️ ERROR: Tidak ditemukan data berlabel\")\n",
    "  pd.DataFrame(columns=['path', 'label'])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Ambil 3 file paling awal\n",
    "image_files = data_files[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i in range(len(image_files)):\n",
    "    img = cv2.imread(image_files[i], cv2.IMREAD_GRAYSCALE)\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(os.path.basename(data_files[i]))\n",
    "    axes[i].axis('off')\n",
    "\n",
    "print(\"\\n   Sampel Data\")\n",
    "plt.suptitle('TB Chest X-Ray ShenzhenDataset', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cltWYpsUf_xF"
   },
   "source": [
    "## Fungsi untuk menampilkan gambar dari folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm8nmzr0cNyZ"
   },
   "outputs": [],
   "source": [
    "def show_images_from_folder(folder_path, title=\"Sample Images\"):\n",
    "    images = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "    if len(images) == 0:\n",
    "        print(f\"Tidak ada gambar di folder {folder_path}\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(3):\n",
    "        img = cv2.imread(images[i], cv2.IMREAD_GRAYSCALE)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(os.path.basename(images[i]))\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmcRkqldgbDZ"
   },
   "source": [
    "## Membuat folder untuk menyimpan gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5EMgLy7gDog"
   },
   "outputs": [],
   "source": [
    "filter_folder = \"filtered_images\"\n",
    "clahe_folder = \"filtered_clahe\"\n",
    "output_folder = \"output_images\"\n",
    "\n",
    "# Cek folder gambar yang sudah ada\n",
    "for folder in [filter_folder, clahe_folder, output_folder]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgW4rsMgymS"
   },
   "source": [
    "## Preprocessing: Gaussian filter dan CLACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03zTY-d9nllk"
   },
   "outputs": [],
   "source": [
    "def preprocessing(img, kernel_size=(5,5)):\n",
    "    if img is None:\n",
    "        return None\n",
    "    # Mengaplikasikan Gaussian\n",
    "    filtered_img = cv2.GaussianBlur(img, kernel_size, 0.5)\n",
    "\n",
    "    # Mengaplikasikan CLAHE untuk contrast enhancement\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced_img = clahe.apply(filtered_img)\n",
    "\n",
    "    return enhanced_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax5gqwB7k8IL"
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr7W00zMlTXo"
   },
   "source": [
    "### Otsu thresholding (sensitive segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18vJtTVahiFR"
   },
   "outputs": [],
   "source": [
    "def otsu_threshold(image):\n",
    "  if image is None:\n",
    "    return None\n",
    "  _, otsu_thresh_img = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "  return otsu_thresh_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq0D716cm9Fy"
   },
   "source": [
    "### Global thresholding (insensitive segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5j3TRlFmKrU"
   },
   "outputs": [],
   "source": [
    "def global_threshold(image, threshold_value=127):\n",
    "  if image is None:\n",
    "    return None\n",
    "  _, global_thresh_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "  return global_thresh_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEnad8eHnFd1"
   },
   "source": [
    "### Morphological processing (closing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx8P6weonH2i"
   },
   "outputs": [],
   "source": [
    "def closing(binary_mask, kernel_size=(5, 5)):\n",
    "  if binary_mask is None:\n",
    "    return None\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "  closed = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
    "  return closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FTuA5fx_GbY"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT5VYeKs_ZBw"
   },
   "source": [
    "### Fitur bentuk (shape fearures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSJK9P_o_UMq"
   },
   "outputs": [],
   "source": [
    "def extract_shape_features(mask):\n",
    "  if mask is None:\n",
    "    return {f'shape_{i}': 0 for i in range(17)} # Return default features if mask is None\n",
    "\n",
    "  # Memberikan label ke komponen yang terhubung\n",
    "  labelled_mask = measure.label(mask > 0)\n",
    "  regions = measure.regionprops(labelled_mask)\n",
    "\n",
    "  if len(regions) == 0:\n",
    "    return {f'shape_{i}': 0 for i in range(17)}\n",
    "\n",
    "  # Mengurutkan region berdasarkan area (asumsikan 2 region dengan area terbesar adalah paru-paru kanan dan kiri)\n",
    "  regions = sorted(regions, key=lambda x: x.area, reverse=True)[:2]\n",
    "\n",
    "  features = {}\n",
    "\n",
    "  if len(regions) >= 2:\n",
    "    r_left, r_right = regions[0], regions[1]\n",
    "\n",
    "    # Fitur 1: tinggi\n",
    "    h_l = r_left.bbox[2] - r_left.bbox[0]\n",
    "    h_r = r_right.bbox[2] - r_right.bbox[0]\n",
    "    features['height_left'] = h_l\n",
    "    features['height_right'] = h_r\n",
    "    features['height_diff'] = abs(h_l - h_r)\n",
    "\n",
    "    # Fitur 2: rasio lebar/tinggi (simplified)\n",
    "    # Ensure h_l and h_r are not zero to prevent division by zero\n",
    "    features['ratio_left_0'] = r_left.major_axis_length / (h_l + 1e-5) if h_l > 0 else 0\n",
    "    features['ratio_right_0'] = r_right.major_axis_length / (h_r + 1e-5) if h_r > 0 else 0\n",
    "    # Only need one ratio for now, or loop it properly. Simplified for existing code structure.\n",
    "    # The original loop for i in range(5) with same values was likely a placeholder or error.\n",
    "    for i in range(1, 5):\n",
    "        features[f'ratio_left_{i}'] = features['ratio_left_0']\n",
    "        features[f'ratio_right_{i}'] = features['ratio_right_0']\n",
    "\n",
    "    # Fitur 3: perimeter (keliling)\n",
    "    features['perimeter_left'] = r_left.perimeter\n",
    "    features['perimeter_right'] = r_right.perimeter\n",
    "\n",
    "    # Fitur 4: eccentricity (\"keanehan\")\n",
    "    features['eccentricity_left'] = r_left.eccentricity\n",
    "    features['eccentricity_right'] = r_right.eccentricity\n",
    "    return features\n",
    "\n",
    "  else: # jika hanya ada 1 region yang terdeteksi\n",
    "    r = regions[0]\n",
    "    h = r.bbox[2] - r.bbox[0]\n",
    "    features['height_left'] = h\n",
    "    features['height_right'] = 0\n",
    "    features['height_diff'] = h\n",
    "\n",
    "    features['ratio_left_0'] = r.major_axis_length / (h + 1e-5) if h > 0 else 0\n",
    "    for i in range(1, 5):\n",
    "        features[f'ratio_left_{i}'] = features['ratio_left_0']\n",
    "        features[f'ratio_right_{i}'] = 0\n",
    "\n",
    "    features['perimeter_left'] = r.perimeter\n",
    "    features['perimeter_right'] = 0\n",
    "\n",
    "    features['eccentricity_left'] = r.eccentricity\n",
    "    features['eccentricity_right'] = 0\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fepE27mccoE6"
   },
   "source": [
    "### First-order Statistical Features (FOFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOFtjODhnfDp"
   },
   "outputs": [],
   "source": [
    "def fofs(image, mask):\n",
    "  # Add initial None checks for image and mask\n",
    "  if image is None or mask is None:\n",
    "    return {k: 0 for k in ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'entropy', 'smoothness', 'uniformity']}\n",
    "\n",
    "  # Apply mask ke gambar\n",
    "  masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "  # Check if masked_image has valid pixels or if the mask is completely empty\n",
    "  if np.all(masked_image == 0) or not np.any(mask > 0):\n",
    "    return {k: 0 for k in ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'entropy', 'smoothness', 'uniformity']}\n",
    "\n",
    "  pixels = masked_image[mask > 0]\n",
    "\n",
    "  if len(pixels) == 0:\n",
    "    return {k: 0 for k in ['mean', 'variance', 'std_dev', 'skewness', 'kurtosis', 'entropy', 'smoothness', 'uniformity']}\n",
    "\n",
    "  # Menghitung fitur statistik\n",
    "  mean = np.mean(pixels)\n",
    "  variance = np.var(pixels)\n",
    "  std_dev = np.std(pixels)\n",
    "  skewness = stats.skew(pixels)\n",
    "  kurtosis = stats.kurtosis(pixels)\n",
    "\n",
    "  # Fitur histogram\n",
    "  hist, _ = np.histogram(pixels, bins=255, range=(0, 256), density=True)\n",
    "  hist = hist[hist > 0]\n",
    "  entropy = -np.sum(hist * np.log2(hist + 1e-10)) # Corrected line\n",
    "  smoothness = 1 - (1 / (1 + variance))\n",
    "  uniformity = np.sum(hist ** 2)\n",
    "\n",
    "  return {\n",
    "      'mean' : mean,\n",
    "      'variance' : variance,\n",
    "      'std_dev' : std_dev,\n",
    "      'skewness' : skewness,\n",
    "      'kurtosis': kurtosis,\n",
    "      'entropy' : entropy,\n",
    "      'smoothness' : smoothness,\n",
    "      'uniformity' : uniformity\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25TfDXesh6Gd"
   },
   "source": [
    "### Gray-Level Co-occurence Matrix (GLCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwUMut1igMEv"
   },
   "outputs": [],
   "source": [
    "def glcm(image, mask):\n",
    "  # Add initial None checks for image and mask\n",
    "  if image is None or mask is None:\n",
    "    return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  # Apply mask to the image\n",
    "  masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "  # Check if masked_image is effectively empty (all zeros) after masking\n",
    "  if np.all(masked_image == 0):\n",
    "      return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  # Normalize the image to 8-bit. Handle cases where normalization might be problematic.\n",
    "  # Ensure there are non-zero pixels before attempting normalization based on min/max.\n",
    "  min_val = np.min(masked_image[masked_image > 0]) if np.any(masked_image > 0) else 0\n",
    "  max_val = np.max(masked_image) if np.any(masked_image > 0) else 0\n",
    "\n",
    "  # If no meaningful pixel data (e.g., all pixels are the same or zero after mask),\n",
    "  # return default features.\n",
    "  if max_val == 0 or min_val == max_val:\n",
    "      return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  # Perform normalization safely\n",
    "  img_8bit = cv2.normalize(masked_image, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n",
    "\n",
    "  # GLCM in 4 directions (0, 45, 90, 135)\n",
    "  distances = [1]\n",
    "  angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "  # Check if img_8bit is suitable for graycomatrix (needs at least 2 unique gray levels for meaningful GLCM)\n",
    "  if img_8bit.size == 0 or len(np.unique(img_8bit)) < 2:\n",
    "      return {f'glcm_{prop}_{angle}' : 0 for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'] for angle in ['0', '45', '90', '135']}\n",
    "\n",
    "  glcm = graycomatrix(img_8bit, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "  # Extract properties\n",
    "  features = {}\n",
    "  properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "\n",
    "  for prop in properties:\n",
    "    values = graycoprops(glcm, prop)\n",
    "    for i, angle in enumerate(['0', '45', '90', '135']):\n",
    "      features[f'glcm_{prop}_{angle}'] = values[0, i]\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPqth8HFpVhS"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So8veWL6mGgA"
   },
   "outputs": [],
   "source": [
    "def process_single_image(image_path):\n",
    "  if not isinstance(image_path, str):\n",
    "    print(\"Invalid path:\", image_path)\n",
    "    return None\n",
    "\n",
    "  if not os.path.exists(image_path):\n",
    "    print(\"Path not found:\", image_path)\n",
    "    return None\n",
    "\n",
    "  # load\n",
    "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "  if img is None:\n",
    "    print(\"OpenCV cannot read:\", image_path)\n",
    "    return None\n",
    "\n",
    "  # 1. Preprocessing\n",
    "  # Pass the loaded image 'img' to preprocessing to avoid redundant loading\n",
    "  enhanced = preprocessing(img)\n",
    "  if enhanced is None:\n",
    "    print(f\"Preprocessing failed for {image_path}\")\n",
    "    return None\n",
    "\n",
    "  # 2. Segmentation (Sensitive)\n",
    "  otsu_mask = otsu_threshold(enhanced)\n",
    "  if otsu_mask is None:\n",
    "    print(f\"Otsu thresholding failed for {image_path}\")\n",
    "    return None\n",
    "  closed_otsu = closing(otsu_mask)\n",
    "  if closed_otsu is None:\n",
    "    print(f\"Closing (Otsu) failed for {image_path}\")\n",
    "    return None\n",
    "\n",
    "  # 3. Segmentation (Insensitive)\n",
    "  global_mask = global_threshold(enhanced)\n",
    "  if global_mask is None:\n",
    "    print(f\"Global thresholding failed for {image_path}\")\n",
    "    return None\n",
    "  closed_global = closing(global_mask)\n",
    "  if closed_global is None:\n",
    "    print(f\"Closing (Global) failed for {image_path}\")\n",
    "    return None\n",
    "\n",
    "  # 4. Feature extraction\n",
    "  # These functions are now expected to return dictionaries (possibly with default/zero values) or handle their own None inputs\n",
    "  shape_featues = extract_shape_features(closed_otsu)\n",
    "  fofs_features = fofs(enhanced, closed_otsu)\n",
    "  glcm_features = glcm(enhanced, closed_otsu)\n",
    "\n",
    "  # Gabungkan semua fitur - now safer as all feature functions should return dicts\n",
    "  all_features = {**shape_featues, **fofs_features, **glcm_features}\n",
    "\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_n9nzsxqpCo"
   },
   "outputs": [],
   "source": [
    "def extract_dataset(df, limit=None):\n",
    "  features_list = []\n",
    "  labels = []\n",
    "\n",
    "  total = len(df) if limit is None else min(limit, len(df))\n",
    "\n",
    "  for idx, row in df.iterrows():\n",
    "    if limit and idx >= limit:\n",
    "      break\n",
    "\n",
    "    if idx % 50 == 0:\n",
    "      print(f\"Processing image {idx+1}/{total}...\")\n",
    "\n",
    "    features = process_single_image(row['path'])\n",
    "    if features is not None:\n",
    "      features_list.append(features)\n",
    "      labels.append(row['label'])\n",
    "\n",
    "  # Konversi ke dataframe\n",
    "  features_df = pd.DataFrame(features_list)\n",
    "  features_df['label'] = labels\n",
    "\n",
    "  print(f\"\\nFeature extraction selesai!\")\n",
    "  print(f\"Total fitur yang diekstrak: {len(features_df.columns)-1}\")\n",
    "\n",
    "  return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_7xPR7EL6Wp"
   },
   "source": [
    "### Klasifikasi hierarkis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtGdRPdkL5YO"
   },
   "outputs": [],
   "source": [
    "class klasifikasiHierarkis:\n",
    "  \"\"\"\n",
    "  Implementasi:\n",
    "  Stage 1: Klasifikasi berdasarkan fitur-fitur bentuk (shape features)\n",
    "  Stage 2: Jika stage 1 = normal, cek dengan semua fitur\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model_type='svm'):\n",
    "    self.model_type = model_type\n",
    "    self.stage1_model = None # Shape feature\n",
    "    self.stage2_model = None # Semua fitur\n",
    "    self.scaler1 = StandardScaler()\n",
    "    self.scaler2 = StandardScaler()\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"\n",
    "    Training model secara dierarkis\n",
    "    X: Dataframe dengan semua fitur\n",
    "    y: label\n",
    "    \"\"\"\n",
    "    # Identifikasi kolom shape features\n",
    "    shape_cols = [col for col in X.columns if 'height' in col or\n",
    "                  'ratio' in col or 'perimeter' in col or 'eccentricity' in col]\n",
    "\n",
    "    X_shape = X[shape_cols]\n",
    "    X_all = X\n",
    "\n",
    "    # Fitur skalar\n",
    "    X_shape_scaled = self.scaler1.fit_transform(X_shape)\n",
    "    X_all_scaled = self.scaler2.fit_transform(X_all)\n",
    "\n",
    "    # Training model Stage 1\n",
    "    if self.model_type == 'svm':\n",
    "      self.stage1_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "      self.stage2_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    else:\n",
    "      self.stage1_model = GaussianNB()\n",
    "      self.stage2_model = GaussianNB()\n",
    "\n",
    "    self.stage1_model.fit(X_shape_scaled, y)\n",
    "    self.stage2_model.fit(X_all_scaled, y)\n",
    "\n",
    "    print(f\"Stage 1 ({self.model_type}) trained on {len(shape_cols)} fitur-fitur bentuk (shape features)\")\n",
    "    print(f\"Stage 2 ({self.model_type}) trained on {X_all.shape[1]} semua fitur\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Prediksi Hierarkis\n",
    "    \"\"\"\n",
    "    shape_cols = [col for col in X.columns if 'height' in col or\n",
    "                   'ratio' in col or 'perimeter' in col or 'eccentricity' in col]\n",
    "\n",
    "    X_shape = X[shape_cols]\n",
    "    X_all = X\n",
    "\n",
    "    X_shape_scaled = self.scaler1.transform(X_shape)\n",
    "    X_all_scaled = self.scaler2.transform(X_all)\n",
    "\n",
    "    # Stage 1: Prediksi dengan fitur-fitur bentuk (shape features)\n",
    "    stage1_pred = self.stage1_model.predict(X_shape_scaled)\n",
    "    stage1_proba = self.stage1_model.predict_proba(X_shape_scaled)[:, 1]\n",
    "\n",
    "    # Stage 2: Untuk case yang diprediksi normal, cek lagi dengan semua fitur\n",
    "    final_pred = stage1_pred.copy()\n",
    "    for i in range(len(stage1_pred)):\n",
    "      if stage1_pred[i] == 0:  # Prediksi = normal\n",
    "        # Gunakan hasil stage 2 untuk prediksi final\n",
    "        stage2_pred = self.stage2_model.predict(X_all_scaled[i:i+1])\n",
    "        final_pred[i] = stage2_pred[0]\n",
    "\n",
    "    return final_pred\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    \"\"\"Get probabilitas prediksi\"\"\"\n",
    "    predictions = self.predict(X)\n",
    "    proba = np.zeros((len(predictions), 2))\n",
    "    proba[predictions == 0, 0] = 1\n",
    "    proba[predictions == 1, 1] = 1\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQJ5oUweSEf2"
   },
   "source": [
    "### Evaluasi dan perbandingan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pnnvLCGQj0H"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred,\n",
    "                               target_names=['Normal', 'TB']))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'TB'],\n",
    "                yticklabels=['Normal', 'TB'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_models(svm_model, nb_model,\n",
    "                svm_path=\"svm_model.pkl\", nb_path=\"nb_model.pkl\"):\n",
    "    with open(svm_path, \"wb\") as f:\n",
    "        pickle.dump(svm_model, f)\n",
    "    with open(nb_path, \"wb\") as f:\n",
    "        pickle.dump(nb_model, f)\n",
    "    print(f\"[INFO] Models saved → {svm_path}, {nb_path}\")\n",
    "\n",
    "def load_models(svm_path=\"svm_model.pkl\", nb_path=\"nb_model.pkl\"):\n",
    "    with open(svm_path, \"rb\") as f:\n",
    "        svm_model = pickle.load(f)\n",
    "    with open(nb_path, \"rb\") as f:\n",
    "        nb_model = pickle.load(f)\n",
    "    print(f\"[INFO] Models loaded ← {svm_path}, {nb_path}\")\n",
    "    return svm_model, nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "31ggrtFOk7vH",
    "outputId": "47b3dcf4-4b60-4558-a09f-c4f48f571679"
   },
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Train hierarchical SVM & NB models with optional feature limit and save them.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TB DETECTION WITH HIERARCHICAL FEATURE EXTRACTION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if not data:\n",
    "        raise RuntimeError(\"Dataset belum dimuat. Jalankan sel import dataset terlebih dahulu.\")\n",
    "\n",
    "    df = pd.DataFrame(data).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    limit = CONFIG.get(\"limit_extract\")\n",
    "    if limit:\n",
    "        print(f\"\\n[INFO] Menggunakan limit ekstraksi fitur: {limit} images\")\n",
    "    else:\n",
    "        print(\"\\n[INFO] Menggunakan seluruh dataset untuk ekstraksi fitur\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXTRACTING FEATURES…\")\n",
    "    print(\"=\" * 60)\n",
    "    features_df = extract_dataset(df, limit=limit)\n",
    "\n",
    "    X = features_df.drop(\"label\", axis=1)\n",
    "    y = features_df[\"label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain set: {len(X_train)} images\")\n",
    "    print(f\"Test set: {len(X_test)} images\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING AND EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    svm_model = klasifikasiHierarkis(model_type=\"svm\")\n",
    "    svm_acc, _ = evaluate_model(\n",
    "        svm_model, X_train, X_test, y_train, y_test, \"SVM (Hierarchical)\"\n",
    "    )\n",
    "\n",
    "    nb_model = klasifikasiHierarkis(model_type=\"nb\")\n",
    "    nb_acc, _ = evaluate_model(\n",
    "        nb_model, X_train, X_test, y_train, y_test, \"Naive Bayes (Hierarchical)\"\n",
    "    )\n",
    "\n",
    "    save_models(svm_model, nb_model)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    comparison_df = pd.DataFrame(\n",
    "        {\"Model\": [\"SVM\", \"Naive Bayes\"], \"Accuracy\": [svm_acc, nb_acc]}\n",
    "    )\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(\n",
    "        comparison_df[\"Model\"],\n",
    "        comparison_df[\"Accuracy\"],\n",
    "        color=[\"#2E86AB\", \"#A23B72\"],\n",
    "    )\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model Performance Comparison\")\n",
    "    plt.ylim([0, 1])\n",
    "    for i, v in enumerate(comparison_df[\"Accuracy\"]):\n",
    "        plt.text(i, v + 0.02, f\"{v:.4f}\", ha=\"center\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
